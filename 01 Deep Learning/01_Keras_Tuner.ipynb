{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "VIuJHjFIdWIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MtKhWRxZ0yoI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import the keras-tuner"
      ],
      "metadata": {
        "id": "5DTNrqRDdtSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO0nXow_dliR",
        "outputId": "57662528-99eb-43c4-f3ee-951f094e222f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m112.6/129.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "r6QhGPJYd6SA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download and prepare the dataset"
      ],
      "metadata": {
        "id": "zesBt_GbeG5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifies image of clothing from the Fashiopn MNIST dataset."
      ],
      "metadata": {
        "id": "xjsvyNx4eQzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data"
      ],
      "metadata": {
        "id": "bZ5YOx5qe2rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egjIpQMneBLe",
        "outputId": "aa894c76-8cfb-4310-c630-801ace07078b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_train[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB-uGmzRezK-",
        "outputId": "dec282c6-e56a-4f81-f64e-d1bba5df04c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(img_train[3], cmap = 'binary')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "fvcbWKehe9r4",
        "outputId": "a7f03404-5ad9-4000-fd1f-d9bbe4128974"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "afd_x_lMfP6o",
        "outputId": "79c3d2b6-526f-4c7e-9d13-c341b58f9638"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-a581e517-bda8-4f3c-a6d2-6e8eb34be3b8\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBUlEQVR4nLXSz0tUURQH8O+597373rx545g6NmNJY6TlJgozEsKFZGCrkDBoVdGmdf9BiwJ3bVr1PwhBUbQfJQcKjWZRTsjo0KDpDPV+eN+997WK0Z2bvtsPfM+Bc4D/HwJycwBZAEAE6zAyfe5RFMQfFYiRAj+CXM/c2HK82VetVMM34RGUmCxz9v7yYnW9dnWyskyHR6azi72Jwep3ScVk9c7LLhKQrpRBSiI2n76puZFT3doUwH4pcmw/zpjrU2zw3dFt4XEWdvbKhpinzTBYt5bDH4qlLYO8cbKBWHOudDHluFtiLrLD0kmM2//6q9VFS+JLLDxv9GzMPV9v3XuzYgHEGSUGCngbRCLd4W6CxPCLHTDwVMkDA0y/qEa/lFFBoF2EEME8CAD6hsZK8+djlmSatuiXXsWfNh27NU6Yelro1bytPElRbaGaO1FGPfc7zPg9HvHlIaVD8AjID9y/+bgZ/6iP9ks7J/QZevh8w/cd2PlGs8CKt92R7MQEk0yA7GtWq9ETN3zRs7fpR7FaWi/3yXaipW1IjFnbaSM70N7dsRzbzbHd8aCx7+wmKskUO5esz0sPmvXYF67gBzoNfxptxb5stxM10iLg1pOTO23NhcUptW1hE6gFYYprC8QMMPNsMM+4pamVbps/HGkSsg+1Cv4d+0Jh//Sm3DjGix4rfwFoJNh2/0cDFgAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-a581e517-bda8-4f3c-a6d2-6e8eb34be3b8 button').onclick = (e) => {\n",
              "        document.querySelector('#id-a581e517-bda8-4f3c-a6d2-6e8eb34be3b8').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-a581e517-bda8-4f3c-a6d2-6e8eb34be3b8 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "1Vns6PaQfUkb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRGH09yIfc3X",
        "outputId": "ad3b834c-4204-4937-fc21-6824b5e8c9e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
              "        0.05098039, 0.28627452, 0.        , 0.        , 0.00392157,\n",
              "        0.01568628, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.00392157, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.        , 0.14117648,\n",
              "        0.53333336, 0.49803922, 0.24313726, 0.21176471, 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568628,\n",
              "        0.        , 0.        , 0.01176471],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "        0.8       , 0.6901961 , 0.5254902 , 0.5647059 , 0.48235294,\n",
              "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.04705882, 0.03921569, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.60784316,\n",
              "        0.9254902 , 0.8117647 , 0.69803923, 0.41960785, 0.6117647 ,\n",
              "        0.6313726 , 0.42745098, 0.2509804 , 0.09019608, 0.3019608 ,\n",
              "        0.50980395, 0.28235295, 0.05882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00392157, 0.        , 0.27058825, 0.8117647 ,\n",
              "        0.8745098 , 0.85490197, 0.84705883, 0.84705883, 0.6392157 ,\n",
              "        0.49803922, 0.4745098 , 0.47843137, 0.57254905, 0.5529412 ,\n",
              "        0.34509805, 0.6745098 , 0.25882354],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.00392157, 0.00392157, 0.        , 0.78431374, 0.9098039 ,\n",
              "        0.9098039 , 0.9137255 , 0.8980392 , 0.8745098 , 0.8745098 ,\n",
              "        0.84313726, 0.8352941 , 0.6431373 , 0.49803922, 0.48235294,\n",
              "        0.76862746, 0.8980392 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.7176471 , 0.88235295,\n",
              "        0.84705883, 0.8745098 , 0.89411765, 0.92156863, 0.8901961 ,\n",
              "        0.8784314 , 0.87058824, 0.8784314 , 0.8666667 , 0.8745098 ,\n",
              "        0.9607843 , 0.6784314 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.75686276, 0.89411765,\n",
              "        0.85490197, 0.8352941 , 0.7764706 , 0.7058824 , 0.83137256,\n",
              "        0.8235294 , 0.827451  , 0.8352941 , 0.8745098 , 0.8627451 ,\n",
              "        0.9529412 , 0.7921569 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.01176471, 0.        , 0.04705882, 0.85882354, 0.8627451 ,\n",
              "        0.83137256, 0.85490197, 0.7529412 , 0.6627451 , 0.8901961 ,\n",
              "        0.8156863 , 0.85490197, 0.8784314 , 0.83137256, 0.8862745 ,\n",
              "        0.77254903, 0.81960785, 0.20392157],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02352941, 0.        , 0.3882353 , 0.95686275, 0.87058824,\n",
              "        0.8627451 , 0.85490197, 0.79607844, 0.7764706 , 0.8666667 ,\n",
              "        0.84313726, 0.8352941 , 0.87058824, 0.8627451 , 0.9607843 ,\n",
              "        0.46666667, 0.654902  , 0.21960784],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.01568628,\n",
              "        0.        , 0.        , 0.21568628, 0.9254902 , 0.89411765,\n",
              "        0.9019608 , 0.89411765, 0.9411765 , 0.9098039 , 0.8352941 ,\n",
              "        0.85490197, 0.8745098 , 0.91764706, 0.8509804 , 0.8509804 ,\n",
              "        0.81960785, 0.36078432, 0.        ],\n",
              "       [0.        , 0.        , 0.00392157, 0.01568628, 0.02352941,\n",
              "        0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.92941177, 0.8862745 , 0.8509804 ,\n",
              "        0.8745098 , 0.87058824, 0.85882354, 0.87058824, 0.8666667 ,\n",
              "        0.84705883, 0.8745098 , 0.8980392 , 0.84313726, 0.85490197,\n",
              "        1.        , 0.3019608 , 0.        ],\n",
              "       [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24313726,\n",
              "        0.5686275 , 0.8       , 0.89411765, 0.8117647 , 0.8352941 ,\n",
              "        0.8666667 , 0.85490197, 0.8156863 , 0.827451  , 0.85490197,\n",
              "        0.8784314 , 0.8745098 , 0.85882354, 0.84313726, 0.8784314 ,\n",
              "        0.95686275, 0.62352943, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
              "        0.17254902, 0.32156864, 0.41960785, 0.7411765 , 0.89411765,\n",
              "        0.8627451 , 0.87058824, 0.8509804 , 0.8862745 , 0.78431374,\n",
              "        0.8039216 , 0.827451  , 0.9019608 , 0.8784314 , 0.91764706,\n",
              "        0.6901961 , 0.7372549 , 0.98039216, 0.972549  , 0.9137255 ,\n",
              "        0.93333334, 0.84313726, 0.        ],\n",
              "       [0.        , 0.22352941, 0.73333335, 0.8156863 , 0.8784314 ,\n",
              "        0.8666667 , 0.8784314 , 0.8156863 , 0.8       , 0.8392157 ,\n",
              "        0.8156863 , 0.81960785, 0.78431374, 0.62352943, 0.9607843 ,\n",
              "        0.75686276, 0.80784315, 0.8745098 , 1.        , 1.        ,\n",
              "        0.8666667 , 0.91764706, 0.8666667 , 0.827451  , 0.8627451 ,\n",
              "        0.9098039 , 0.9647059 , 0.        ],\n",
              "       [0.01176471, 0.7921569 , 0.89411765, 0.8784314 , 0.8666667 ,\n",
              "        0.827451  , 0.827451  , 0.8392157 , 0.8039216 , 0.8039216 ,\n",
              "        0.8039216 , 0.8627451 , 0.9411765 , 0.3137255 , 0.5882353 ,\n",
              "        1.        , 0.8980392 , 0.8666667 , 0.7372549 , 0.6039216 ,\n",
              "        0.7490196 , 0.8235294 , 0.8       , 0.81960785, 0.87058824,\n",
              "        0.89411765, 0.88235295, 0.        ],\n",
              "       [0.38431373, 0.9137255 , 0.7764706 , 0.8235294 , 0.87058824,\n",
              "        0.8980392 , 0.8980392 , 0.91764706, 0.9764706 , 0.8627451 ,\n",
              "        0.7607843 , 0.84313726, 0.8509804 , 0.94509804, 0.25490198,\n",
              "        0.28627452, 0.41568628, 0.45882353, 0.65882355, 0.85882354,\n",
              "        0.8666667 , 0.84313726, 0.8509804 , 0.8745098 , 0.8745098 ,\n",
              "        0.8784314 , 0.8980392 , 0.11372549],\n",
              "       [0.29411766, 0.8       , 0.83137256, 0.8       , 0.75686276,\n",
              "        0.8039216 , 0.827451  , 0.88235295, 0.84705883, 0.7254902 ,\n",
              "        0.77254903, 0.80784315, 0.7764706 , 0.8352941 , 0.9411765 ,\n",
              "        0.7647059 , 0.8901961 , 0.9607843 , 0.9372549 , 0.8745098 ,\n",
              "        0.85490197, 0.83137256, 0.81960785, 0.87058824, 0.8627451 ,\n",
              "        0.8666667 , 0.9019608 , 0.2627451 ],\n",
              "       [0.1882353 , 0.79607844, 0.7176471 , 0.7607843 , 0.8352941 ,\n",
              "        0.77254903, 0.7254902 , 0.74509805, 0.7607843 , 0.7529412 ,\n",
              "        0.7921569 , 0.8392157 , 0.85882354, 0.8666667 , 0.8627451 ,\n",
              "        0.9254902 , 0.88235295, 0.84705883, 0.78039217, 0.80784315,\n",
              "        0.7294118 , 0.70980394, 0.69411767, 0.6745098 , 0.70980394,\n",
              "        0.8039216 , 0.80784315, 0.4509804 ],\n",
              "       [0.        , 0.47843137, 0.85882354, 0.75686276, 0.7019608 ,\n",
              "        0.67058825, 0.7176471 , 0.76862746, 0.8       , 0.8235294 ,\n",
              "        0.8352941 , 0.8117647 , 0.827451  , 0.8235294 , 0.78431374,\n",
              "        0.76862746, 0.7607843 , 0.7490196 , 0.7647059 , 0.7490196 ,\n",
              "        0.7764706 , 0.7529412 , 0.6901961 , 0.6117647 , 0.654902  ,\n",
              "        0.69411767, 0.8235294 , 0.36078432],\n",
              "       [0.        , 0.        , 0.2901961 , 0.7411765 , 0.83137256,\n",
              "        0.7490196 , 0.6862745 , 0.6745098 , 0.6862745 , 0.70980394,\n",
              "        0.7254902 , 0.7372549 , 0.7411765 , 0.7372549 , 0.75686276,\n",
              "        0.7764706 , 0.8       , 0.81960785, 0.8235294 , 0.8235294 ,\n",
              "        0.827451  , 0.7372549 , 0.7372549 , 0.7607843 , 0.7529412 ,\n",
              "        0.84705883, 0.6666667 , 0.        ],\n",
              "       [0.00784314, 0.        , 0.        , 0.        , 0.25882354,\n",
              "        0.78431374, 0.87058824, 0.92941177, 0.9372549 , 0.9490196 ,\n",
              "        0.9647059 , 0.9529412 , 0.95686275, 0.8666667 , 0.8627451 ,\n",
              "        0.75686276, 0.7490196 , 0.7019608 , 0.7137255 , 0.7137255 ,\n",
              "        0.70980394, 0.6901961 , 0.6509804 , 0.65882355, 0.3882353 ,\n",
              "        0.22745098, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "        0.28235295, 0.16078432, 0.13725491, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### define the Model"
      ],
      "metadata": {
        "id": "OjxKewxhfiwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  #input layers\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))#output layer\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Ab_5fkVzffJP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instantiate the tuner and perform hypertuning"
      ],
      "metadata": {
        "id": "T29Y5sYUfuCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')"
      ],
      "metadata": {
        "id": "MUlq29OffstP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "8DSIGtyRf7zz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BslLx985f_kU",
        "outputId": "afc6f612-4a10-4404-970a-86a073fa493e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 01m 30s]\n",
            "val_accuracy: 0.8744999766349792\n",
            "\n",
            "Best val_accuracy So Far: 0.8926666378974915\n",
            "Total elapsed time: 00h 22m 54s\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
            "layer is 224 and the optimal learning rate for the optimizer\n",
            "is 0.001.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "XF7o-AoXhHhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJLf_dc6gD1J",
        "outputId": "cb42cadc-cf8e-4652-cb5b-2cfed7711d29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 9s 5ms/step - loss: 0.5027 - accuracy: 0.8229 - val_loss: 0.4016 - val_accuracy: 0.8577\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.3754 - accuracy: 0.8640 - val_loss: 0.3879 - val_accuracy: 0.8577\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3353 - accuracy: 0.8760 - val_loss: 0.3314 - val_accuracy: 0.8754\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.3087 - accuracy: 0.8853 - val_loss: 0.3525 - val_accuracy: 0.8718\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2915 - accuracy: 0.8926 - val_loss: 0.3268 - val_accuracy: 0.8832\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2751 - accuracy: 0.8982 - val_loss: 0.3163 - val_accuracy: 0.8844\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2616 - accuracy: 0.9035 - val_loss: 0.3351 - val_accuracy: 0.8815\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2500 - accuracy: 0.9061 - val_loss: 0.3191 - val_accuracy: 0.8878\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2396 - accuracy: 0.9101 - val_loss: 0.3174 - val_accuracy: 0.8873\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2309 - accuracy: 0.9134 - val_loss: 0.3292 - val_accuracy: 0.8856\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2200 - accuracy: 0.9179 - val_loss: 0.3072 - val_accuracy: 0.8911\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2121 - accuracy: 0.9205 - val_loss: 0.3480 - val_accuracy: 0.8784\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2059 - accuracy: 0.9227 - val_loss: 0.3126 - val_accuracy: 0.8948\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2003 - accuracy: 0.9245 - val_loss: 0.3324 - val_accuracy: 0.8882\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1918 - accuracy: 0.9276 - val_loss: 0.3156 - val_accuracy: 0.8936\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1850 - accuracy: 0.9315 - val_loss: 0.3697 - val_accuracy: 0.8817\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1794 - accuracy: 0.9331 - val_loss: 0.3327 - val_accuracy: 0.8930\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1723 - accuracy: 0.9356 - val_loss: 0.3247 - val_accuracy: 0.8911\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1691 - accuracy: 0.9365 - val_loss: 0.3446 - val_accuracy: 0.8939\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1622 - accuracy: 0.9398 - val_loss: 0.3567 - val_accuracy: 0.8904\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1587 - accuracy: 0.9403 - val_loss: 0.3717 - val_accuracy: 0.8865\n",
            "Epoch 22/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1544 - accuracy: 0.9420 - val_loss: 0.3511 - val_accuracy: 0.8917\n",
            "Epoch 23/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1506 - accuracy: 0.9438 - val_loss: 0.3466 - val_accuracy: 0.8938\n",
            "Epoch 24/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1476 - accuracy: 0.9457 - val_loss: 0.3822 - val_accuracy: 0.8913\n",
            "Epoch 25/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1388 - accuracy: 0.9473 - val_loss: 0.3580 - val_accuracy: 0.8953\n",
            "Epoch 26/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1388 - accuracy: 0.9487 - val_loss: 0.3618 - val_accuracy: 0.8927\n",
            "Epoch 27/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1359 - accuracy: 0.9494 - val_loss: 0.3649 - val_accuracy: 0.8942\n",
            "Epoch 28/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1332 - accuracy: 0.9505 - val_loss: 0.4065 - val_accuracy: 0.8855\n",
            "Epoch 29/50\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1266 - accuracy: 0.9525 - val_loss: 0.4014 - val_accuracy: 0.8884\n",
            "Epoch 30/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1252 - accuracy: 0.9532 - val_loss: 0.4101 - val_accuracy: 0.8910\n",
            "Epoch 31/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1209 - accuracy: 0.9550 - val_loss: 0.4041 - val_accuracy: 0.8931\n",
            "Epoch 32/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1212 - accuracy: 0.9541 - val_loss: 0.4290 - val_accuracy: 0.8901\n",
            "Epoch 33/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1145 - accuracy: 0.9580 - val_loss: 0.4258 - val_accuracy: 0.8907\n",
            "Epoch 34/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1149 - accuracy: 0.9569 - val_loss: 0.4296 - val_accuracy: 0.8831\n",
            "Epoch 35/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1102 - accuracy: 0.9581 - val_loss: 0.4386 - val_accuracy: 0.8918\n",
            "Epoch 36/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1078 - accuracy: 0.9600 - val_loss: 0.4346 - val_accuracy: 0.8898\n",
            "Epoch 37/50\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1071 - accuracy: 0.9601 - val_loss: 0.4292 - val_accuracy: 0.8924\n",
            "Epoch 38/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1012 - accuracy: 0.9626 - val_loss: 0.4571 - val_accuracy: 0.8871\n",
            "Epoch 39/50\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1027 - accuracy: 0.9617 - val_loss: 0.4314 - val_accuracy: 0.8892\n",
            "Epoch 40/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0983 - accuracy: 0.9639 - val_loss: 0.4818 - val_accuracy: 0.8908\n",
            "Epoch 41/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1006 - accuracy: 0.9628 - val_loss: 0.4605 - val_accuracy: 0.8925\n",
            "Epoch 42/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0934 - accuracy: 0.9654 - val_loss: 0.4666 - val_accuracy: 0.8917\n",
            "Epoch 43/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0939 - accuracy: 0.9641 - val_loss: 0.4583 - val_accuracy: 0.8910\n",
            "Epoch 44/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0928 - accuracy: 0.9652 - val_loss: 0.5085 - val_accuracy: 0.8832\n",
            "Epoch 45/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0892 - accuracy: 0.9672 - val_loss: 0.4835 - val_accuracy: 0.8917\n",
            "Epoch 46/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0878 - accuracy: 0.9674 - val_loss: 0.4878 - val_accuracy: 0.8882\n",
            "Epoch 47/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0887 - accuracy: 0.9667 - val_loss: 0.5291 - val_accuracy: 0.8872\n",
            "Epoch 48/50\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0830 - accuracy: 0.9687 - val_loss: 0.4994 - val_accuracy: 0.8893\n",
            "Epoch 49/50\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0833 - accuracy: 0.9688 - val_loss: 0.5058 - val_accuracy: 0.8926\n",
            "Epoch 50/50\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0814 - accuracy: 0.9698 - val_loss: 0.5068 - val_accuracy: 0.8917\n",
            "Best epoch: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
      ],
      "metadata": {
        "id": "2Vw11i-lhTHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEoSsSEyhg0J",
        "outputId": "c5fddc6d-da0b-45af-f0d5-6f9656859a71"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.5038 - accuracy: 0.8238 - val_loss: 0.4125 - val_accuracy: 0.8526\n",
            "Epoch 2/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.3780 - accuracy: 0.8635 - val_loss: 0.3881 - val_accuracy: 0.8567\n",
            "Epoch 3/25\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3369 - accuracy: 0.8766 - val_loss: 0.3642 - val_accuracy: 0.8677\n",
            "Epoch 4/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.3109 - accuracy: 0.8860 - val_loss: 0.3668 - val_accuracy: 0.8662\n",
            "Epoch 5/25\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2938 - accuracy: 0.8919 - val_loss: 0.3399 - val_accuracy: 0.8760\n",
            "Epoch 6/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2751 - accuracy: 0.8977 - val_loss: 0.3294 - val_accuracy: 0.8825\n",
            "Epoch 7/25\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2620 - accuracy: 0.9024 - val_loss: 0.3271 - val_accuracy: 0.8786\n",
            "Epoch 8/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2522 - accuracy: 0.9065 - val_loss: 0.3248 - val_accuracy: 0.8840\n",
            "Epoch 9/25\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2399 - accuracy: 0.9104 - val_loss: 0.3139 - val_accuracy: 0.8878\n",
            "Epoch 10/25\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2295 - accuracy: 0.9134 - val_loss: 0.3550 - val_accuracy: 0.8723\n",
            "Epoch 11/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2210 - accuracy: 0.9183 - val_loss: 0.3112 - val_accuracy: 0.8903\n",
            "Epoch 12/25\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2133 - accuracy: 0.9194 - val_loss: 0.3154 - val_accuracy: 0.8924\n",
            "Epoch 13/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2054 - accuracy: 0.9230 - val_loss: 0.3493 - val_accuracy: 0.8814\n",
            "Epoch 14/25\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2002 - accuracy: 0.9248 - val_loss: 0.3312 - val_accuracy: 0.8889\n",
            "Epoch 15/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1900 - accuracy: 0.9270 - val_loss: 0.3240 - val_accuracy: 0.8933\n",
            "Epoch 16/25\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1845 - accuracy: 0.9317 - val_loss: 0.3358 - val_accuracy: 0.8882\n",
            "Epoch 17/25\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1789 - accuracy: 0.9314 - val_loss: 0.3348 - val_accuracy: 0.8923\n",
            "Epoch 18/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1728 - accuracy: 0.9356 - val_loss: 0.3370 - val_accuracy: 0.8887\n",
            "Epoch 19/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1661 - accuracy: 0.9383 - val_loss: 0.3447 - val_accuracy: 0.8945\n",
            "Epoch 20/25\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1639 - accuracy: 0.9384 - val_loss: 0.3454 - val_accuracy: 0.8949\n",
            "Epoch 21/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1612 - accuracy: 0.9391 - val_loss: 0.3474 - val_accuracy: 0.8932\n",
            "Epoch 22/25\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1550 - accuracy: 0.9417 - val_loss: 0.3521 - val_accuracy: 0.8943\n",
            "Epoch 23/25\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1498 - accuracy: 0.9443 - val_loss: 0.3646 - val_accuracy: 0.8942\n",
            "Epoch 24/25\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1427 - accuracy: 0.9462 - val_loss: 0.3968 - val_accuracy: 0.8923\n",
            "Epoch 25/25\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1417 - accuracy: 0.9470 - val_loss: 0.3615 - val_accuracy: 0.8951\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7df819986650>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the hypermodel on the test data"
      ],
      "metadata": {
        "id": "7mTUf0Irhlj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = model.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "strxN8LChwzX",
        "outputId": "59b63cd9-4629-46f6-c711-32334cc6e150"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5751 - accuracy: 0.8878\n",
            "[test loss, test accuracy]: [0.5750709176063538, 0.8877999782562256]\n"
          ]
        }
      ]
    }
  ]
}